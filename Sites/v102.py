import aiohttp
import asyncio
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urljoin
from config import KEYWORDS, EXCLUDED_KEYWORDS

BASE_URL = "https://v102.ru/"

def is_valid_news(title):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å: —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞"""
    title_lower = title.lower()
    return any(keyword.lower() in title_lower for keyword in KEYWORDS) and not any(
        excluded.lower() in title_lower for excluded in EXCLUDED_KEYWORDS
    )

async def fetch_news_datetime(session, news_url):
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏ —Å V102.ru"""
    try:
        async with session.get(news_url, ssl=False) as response:
            response.raise_for_status()
            soup = BeautifulSoup(await response.text(), "lxml")

            # –ò—â–µ–º –¥–∞—Ç—É –≤ <span class="mobile-date">
            date_span = soup.find("span", class_="mobile-date")
            if date_span:
                raw_date = date_span.text.strip()  # –ù–∞–ø—Ä–∏–º–µ—Ä, "25.03.2025 20:29"
                news_time = datetime.strptime(raw_date, "%d.%m.%Y %H:%M")  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ datetime
                return news_time
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞—Ç—ã: {e}")
    return None

async def fetch_v102_news():
    """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å V102.ru"""
    news_list = []
    yesterday = datetime.now() - timedelta(days=1)
    headers = {"User-Agent": "Mozilla/5.0"}
    timeout = aiohttp.ClientTimeout(total=60)

    async with aiohttp.ClientSession(headers=headers, timeout=timeout) as session:
        try:
            async with session.get(BASE_URL, ssl=False) as response:
                response.raise_for_status()
                soup = BeautifulSoup(await response.text(), "lxml")

                # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
                articles = soup.find_all("a", href=True)

                for article in articles:
                    link = urljoin(BASE_URL, article["href"])
                    title = article.get_text(strip=True)

                    if title and is_valid_news(title):
                        news_date = await fetch_news_datetime(session, link)
                        if news_date and news_date >= yesterday:
                            news_list.append({
                                "title": title,
                                "link": link,
                                "date": news_date.strftime("%d.%m.%y %H:%M")
                            })
        except aiohttp.ClientError as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}")
    return news_list

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    v102_news = await fetch_v102_news()

    if not v102_news:
        print(f"‚ùóÔ∏è –î–ª—è —Å–∞–π—Ç–∞ {BASE_URL} –Ω–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º!")
        print(f"{'-' * 50}")
        return

    seen_links = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    seen_titles = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤

    for news in v102_news:
        if news['link'] not in seen_links and news['title'] not in seen_titles:
            seen_links.add(news['link'])  # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ
            seen_titles.add(news['title'])  # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ
            print(
                f"üì¢ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {news['title']}\n"
                f"üîó –°—Å—ã–ª–∫–∞: {news['link']}\n"
                f"üìÖ –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {news['date']}\n"
                f"{'-' * 50}"
            )

if __name__ == "__main__":
    asyncio.run(main())