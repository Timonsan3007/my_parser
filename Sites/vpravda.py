import aiohttp
import asyncio
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urljoin
from config import KEYWORDS, EXCLUDED_KEYWORDS


def is_valid_news(title):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å: —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ–Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞.
    """
    title_lower = title.lower()
    return (
            any(keyword.lower() in title_lower for keyword in KEYWORDS) and
            not any(excluded.lower() in title_lower for excluded in EXCLUDED_KEYWORDS)
    )


def is_valid_link(link):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –¥–æ–ø—É—Å—Ç–∏–º–æ–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞.
    –ò—Å–∫–ª—é—á–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ —Ä–µ—Å—É—Ä—Å—ã –∏ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä—ã.
    """
    invalid_protocols = ["tel:", "whatsapp:", "viber:", "tg:", "mailto:"]
    invalid_domains = ["youtube.com", "youtu.be"]  # –î–æ–±–∞–≤—å—Ç–µ –¥—Ä—É–≥–∏–µ –¥–æ–º–µ–Ω—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ

    if any(link.startswith(protocol) for protocol in invalid_protocols):
        return False

    if any(domain in link for domain in invalid_domains):
        return False

    return True


async def fetch_news_datetime(session, news_url):
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–∞-—Ç–µ–≥–æ–≤ –∏–ª–∏ —Ç–µ–≥–∞ <time>.
    """
    try:
        async with session.get(news_url, ssl=False) as response:
            response.raise_for_status()
            soup = BeautifulSoup(await response.text(), "lxml")

            # –ò—â–µ–º –º–µ—Ç–∞-—Ç–µ–≥ —Å –≤—Ä–µ–º–µ–Ω–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            meta_time = soup.find("meta", {"property": "article:published_time"})
            if meta_time and meta_time.get("content"):
                return datetime.fromisoformat(meta_time["content"][:19])

            # –ï—Å–ª–∏ –º–µ—Ç–∞-—Ç–µ–≥ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏—â–µ–º —Ç–µ–≥ <time>
            date_tag = soup.find("time")
            if date_tag and date_tag.get("datetime"):
                return datetime.fromisoformat(date_tag["datetime"][:19])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")

    return None

async def fetch_vpravda_news():
    """
    –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å–∞–π—Ç–∞ https://vpravda.ru/ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏.
    """
    base_url = "https://vpravda.ru/"
    news_list = []
    yesterday = datetime.now().date() - timedelta(days=1)
    headers = {"User-Agent": "Mozilla/5.0"}
    timeout = aiohttp.ClientTimeout(total=60)

    async with aiohttp.ClientSession(headers=headers, timeout=timeout) as session:
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –≤—á–µ—Ä–∞—à–Ω–∏–π –¥–µ–Ω—å
            url = f"{base_url}?dateFrom={yesterday.strftime('%d.%m.%Y')}"
            async with session.get(url, ssl=False) as response:
                response.raise_for_status()
                soup = BeautifulSoup(await response.text(), "lxml")

                # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
                articles = soup.find_all("a", href=True)
                for article in articles:
                    title = article.text.strip()
                    link = urljoin(base_url, article["href"])

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å—Å—ã–ª–∫–∏
                    if not is_valid_link(link):
                        continue

                    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏
                    news_date = await fetch_news_datetime(session, link)
                    if (
                            news_date and
                            news_date.date() >= yesterday and
                            is_valid_news(title)
                    ):
                        news_list.append({
                            "title": title,
                            "link": link,
                            "date": news_date.strftime("%d.%m.%y %H:%M"),
                            "datetime": news_date  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–∫—Ç datetime –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
                        })
        except aiohttp.ClientError as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ —Å–∞–π—Ç—É: {e}")

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–æ—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º)
    news_list.sort(key=lambda x: x["datetime"], reverse=True)

    return news_list


async def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.
    """
    vpravda_news = await fetch_vpravda_news()

    if not vpravda_news:
        print(f"‚ùóÔ∏è –î–ª—è —Å–∞–π—Ç–∞ https://vpravda.ru/ –Ω–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º!")
        print(f"{'-' * 50}")
        return

    seen_links = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    seen_titles = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤

    for news in vpravda_news:
        if news['link'] not in seen_links and news['title'] not in seen_titles:
            seen_links.add(news['link'])
            seen_titles.add(news['title'])
            print(
                f"üì¢ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {news['title']}\n"
                f"üîó –°—Å—ã–ª–∫–∞: {news['link']}\n"
                f"üìÖ –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {news['date']}\n"
                f"{'-' * 50}"
            )


if __name__ == "__main__":
    asyncio.run(main())