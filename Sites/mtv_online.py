import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlsplit
from config import KEYWORDS, EXCLUDED_KEYWORDS  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ config.py
import asyncio
from datetime import datetime

# –ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞
base_url = 'https://–º—Ç–≤.–æ–Ω–ª–∞–π–Ω/feed'

def make_clickable_url(url):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç URL –≤ –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (Punycode)"""
    try:
        parsed = urlsplit(url)
        if any(ord(c) > 127 for c in parsed.netloc):
            netloc = parsed.netloc.encode('idna').decode('ascii')
            parsed = parsed._replace(netloc=netloc)
        return parsed.geturl()
    except Exception:
        return url

def contains_keywords(text, keywords):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"""
    return any(word.lower() in text.lower() for word in keywords)

MONTHS_RU = {
    "—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4,
    "–º–∞—è": 5, "–∏—é–Ω—è": 6, "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8,
    "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12
}

def format_date(date_str):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞—Ç—É –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ '6 –ê–ø—Ä–µ–ª—è, 14:55' –≤ —Ñ–æ—Ä–º–∞—Ç '06.04.25 14:55'.
    """
    try:
        if not date_str:
            return None

        parts = date_str.split(", ")
        if len(parts) != 2:
            return None

        date_part, time_part = parts
        day, month_name = date_part.split(" ")
        month = MONTHS_RU.get(month_name.lower())
        if not month:
            return None

        year = datetime.now().year % 100
        formatted_date = f"{int(day):02}.{month:02}.{year:02} {time_part}"
        return formatted_date
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–∞—Ç—ã '{date_str}': {e}")
        return None

async def fetch_mtv_news():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å–∞–π—Ç–∞ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∏—Ö –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    news_list = []
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get('https://xn--b1ats.xn--80asehdb/feed', ssl=False) as response:
                response.raise_for_status()
                soup = BeautifulSoup(await response.text(), 'html.parser')

                for item in soup.find_all('div', class_='item-title'):
                    title_tag = item.find('h2')
                    title = title_tag.text.strip() if title_tag else ""

                    link_tag = item.find('a', href=True)
                    if not link_tag:
                        continue
                    relative_link = link_tag['href']
                    absolute_link = urljoin(base_url, relative_link)
                    clickable_link = make_clickable_url(absolute_link)

                    description = ""
                    summary_block = item.find_next_sibling('p', class_='short')
                    if summary_block:
                        description = summary_block.text.strip()

                    date = None
                    summary_block = item.find_next_sibling('div', class_='summary')
                    if summary_block:
                        date_tag = summary_block.find('span', class_='dt')
                        if date_tag:
                            raw_date = date_tag.text.strip().split('|')[0].strip()
                            date = format_date(raw_date)

                    full_text = f"{title} {description}"
                    if contains_keywords(full_text, KEYWORDS) and not contains_keywords(full_text, EXCLUDED_KEYWORDS):
                        news_list.append({
                            "title": title,
                            "link": clickable_link,
                            "date": date
                        })

    except Exception as e:
        print(f"‚ùóÔ∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")

    return news_list

def display_news(news_list):
    """–í—ã–≤–æ–¥–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ –≤ —Ç—Ä–µ–±—É–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    if not news_list:
        print(f"‚ùóÔ∏è –î–ª—è —Å–∞–π—Ç–∞ {base_url} –Ω–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º!")
        print(f"{'-' * 50}")
        return

    seen_links = set()
    for news in news_list:
        if news['link'] not in seen_links:
            seen_links.add(news['link'])
            print(
                f"üì¢ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {news['title']}\n"
                f"üîó –°—Å—ã–ª–∫–∞: {news['link']}\n"
                f"üìÖ –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {news['date']}\n"
                f"{'-' * 50}"
            )

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    news = await fetch_mtv_news()
    display_news(news)

if __name__ == "__main__":
    asyncio.run(main())