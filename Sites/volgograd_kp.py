import aiohttp
import asyncio
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urljoin
from config import KEYWORDS, EXCLUDED_KEYWORDS
import pytz

async def fetch_news_datetime(session, news_url):
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏ –≤ —á–∞—Å–æ–≤–æ–º –ø–æ—è—Å–µ –í–æ–ª–≥–æ–≥—Ä–∞–¥–∞"""
    try:
        async with session.get(news_url, ssl=False) as response:
            response.raise_for_status()
            soup = BeautifulSoup(await response.text(), "lxml")
            meta_time = soup.find("meta", {"property": "article:published_time"})
            if meta_time and meta_time.get("content"):
                news_time = datetime.fromisoformat(meta_time["content"][:19])  # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç datetime –±–µ–∑ —É—á–µ—Ç–∞ —á–∞—Å–æ–≤–æ–≥–æ –ø–æ—è—Å–∞
                utc_time = news_time.replace(tzinfo=pytz.utc)  # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º UTC
                volgograd_time = utc_time.astimezone(pytz.timezone("Europe/Volgograd"))  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫–æ–µ –≤—Ä–µ–º—è
                return volgograd_time
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞—Ç—ã: {e}")
    return None


BASE_URL = "https://www.volgograd.kp.ru/online/"  # –†–∞–∑–¥–µ–ª —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏

def is_valid_news(title):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å: —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞"""
    title_lower = title.lower()
    return any(keyword.lower() in title_lower for keyword in KEYWORDS) and not any(
        excluded.lower() in title_lower for excluded in EXCLUDED_KEYWORDS)

async def fetch_kp_news():
    """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Å–∞–π—Ç–∞ –ö–ü-–í–æ–ª–≥–æ–≥—Ä–∞–¥"""
    news_list = []
    yesterday = datetime.now().date() - timedelta(days=1)
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept-Language": "ru-RU,ru;q=0.9",
    }
    timeout = aiohttp.ClientTimeout(total=60)

    async with aiohttp.ClientSession(headers=headers, timeout=timeout) as session:
        try:
            async with session.get(BASE_URL, ssl=False) as response:
                response.raise_for_status()
                soup = BeautifulSoup(await response.text(), "lxml")

                # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
                articles = soup.find_all("a", href=True)

                for article in articles:
                    link = urljoin(BASE_URL, article["href"])
                    title = article.get_text(strip=True)

                    if title and is_valid_news(title):
                        news_date = await fetch_news_datetime(session, link)
                        if news_date and news_date.date() >= yesterday:
                            news_list.append({
                                "title": title,
                                "link": link,
                                "date": news_date.strftime("%d.%m.%y %H:%M")
                            })
        except aiohttp.ClientError as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}")
    return news_list

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    kp_news = await fetch_kp_news()

    if not kp_news:
        print(f"‚ùóÔ∏è –î–ª—è —Å–∞–π—Ç–∞ {BASE_URL} –Ω–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º!")
        print(f"{'-' * 50}")
        return

    seen_links = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    seen_titles = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤

    for news in kp_news:
        if news['link'] not in seen_links and news['title'] not in seen_titles:
            seen_links.add(news['link'])  # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ
            seen_titles.add(news['title'])  # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ
            print(
                f"üì¢ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {news['title']}\n"
                f"üîó –°—Å—ã–ª–∫–∞: {news['link']}\n"
                f"üìÖ –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {news['date']}\n"
                f"{'-' * 50}"
            )


if __name__ == "__main__":
    asyncio.run(main())