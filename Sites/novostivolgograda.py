import aiohttp
import asyncio
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urljoin
from config import KEYWORDS, EXCLUDED_KEYWORDS
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def is_valid_news(title):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    if not title:
        return False

    title_lower = title.lower()
    has_keyword = any(keyword.lower() in title_lower for keyword in KEYWORDS)
    no_excluded = not any(excluded.lower() in title_lower for excluded in EXCLUDED_KEYWORDS)

    logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞: '{title}'")
    logger.debug(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {has_keyword}, –ò—Å–∫–ª—é—á–µ–Ω–∏—è: {not no_excluded}")

    return has_keyword and no_excluded


async def fetch_news_datetime(session, news_url):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–∞—Ç—ã —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏"""
    try:
        async with session.get(news_url, ssl=False) as response:
            response.raise_for_status()
            soup = BeautifulSoup(await response.text(), 'lxml')

            # 1. –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∏–∑ JSON
            script_tag = soup.find('script', {'id': '__NEXT_DATA__'})
            if script_tag:
                try:
                    data = json.loads(script_tag.string)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—É—Ç–µ–π –∫ –¥–∞—Ç–µ
                    date_str = (data.get('props', {}).get('pageProps', {}).get('post', {}).get('date') or
                                data.get('props', {}).get('pageProps', {}).get('initialMatters', [{}])[0].get(
                                    'datePublished'))
                    if date_str:
                        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")

            # 2. –ü–æ–ø—Ä–æ–±—É–µ–º —Ä—É—Å—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã
            date_div = soup.select_one('div.MatterTop_date__mPSNt, [class*="date"], [class*="Date"]')
            if date_div:
                date_text = date_div.get_text(strip=True)
                try:
                    months = {
                        '—è–Ω–≤–∞—Ä': 1, '—Ñ–µ–≤—Ä–∞–ª': 2, '–º–∞—Ä—Ç': 3, '–∞–ø—Ä–µ–ª': 4,
                        '–º–∞—è': 5, '–∏—é–Ω': 6, '–∏—é–ª': 7, '–∞–≤–≥—É—Å—Ç': 8,
                        '—Å–µ–Ω—Ç—è–±—Ä': 9, '–æ–∫—Ç—è–±—Ä': 10, '–Ω–æ—è–±—Ä': 11, '–¥–µ–∫–∞–±—Ä': 12
                    }
                    parts = date_text.lower().replace(',', '').split()
                    day = int(parts[0])
                    month_name = next((m for m in months if parts[1].startswith(m)), None)
                    if month_name:
                        month = months[month_name]
                        year = int(parts[2])
                        time_part = parts[3] if len(parts) > 3 else '00:00'
                        hour, minute = map(int, time_part.split(':'))
                        return datetime(year, month, day, hour, minute)
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä—É—Å—Å–∫–æ–π –¥–∞—Ç—ã: {e}")

            # 3. –ü–æ–ø—Ä–æ–±—É–µ–º –º–µ—Ç–∞-—Ç–µ–≥–∏
            for meta in soup.find_all('meta'):
                if meta.get('property') in ['article:published_time', 'pubdate'] and meta.get('content'):
                    try:
                        return datetime.fromisoformat(meta['content'][:19])
                    except ValueError:
                        continue

            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–∞—Ç—É –¥–ª—è {news_url}")
            return None

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞—Ç—ã: {e}")
        return None


async def fetch_novostivolgograda_news():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    base_url = "https://novostivolgograda.ru/news"
    news_list = []
    date_threshold = datetime.now() - timedelta(days=1)

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        "Accept": "text/html,application/xhtml+xml"
    }

    async with aiohttp.ClientSession(headers=headers) as session:
        try:
            async with session.get(base_url, ssl=False) as response:
                response.raise_for_status()
                soup = BeautifulSoup(await response.text(), 'lxml')

                # –ë–æ–ª–µ–µ –≥–∏–±–∫–∏–π –ø–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤
                news_items = soup.select('a[href^="/news/"]')

                for item in news_items:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª—è —Ç–µ—Å—Ç–∞
                    try:
                        title = item.get_text(strip=True)
                        if not title or len(title) < 10:  # –§–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–∞
                            continue

                        link = urljoin(base_url, item['href'])

                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                        if any(n['link'] == link for n in news_list):
                            continue

                        # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É
                        pub_date = await fetch_news_datetime(session, link)
                        if not pub_date:
                            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–∞—Ç—É –¥–ª—è: {title}")
                            continue

                        logger.debug(f"–î–∞—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏: {pub_date} - {title}")

                        if pub_date >= date_threshold and is_valid_news(title):
                            news_list.append({
                                'title': title,
                                'link': link,
                                'date': pub_date.strftime('%d.%m.%y %H:%M')  # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É
                            })

                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
                        continue

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")

    return news_list


async def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤ –ø—Ä–æ–≥—Ä–∞–º–º—É"""
    news = await fetch_novostivolgograda_news()

    if not news:
        print(f"‚ùóÔ∏è –î–ª—è —Å–∞–π—Ç–∞ https://novostivolgograda.ru –Ω–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º!")
        print(f"{'-' * 50}")
        return

    seen_links = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    seen_titles = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    for item in news:
        if item['link'] not in seen_links and item['title'] not in seen_titles:
            seen_links.add(item['link'])
            seen_titles.add(item['title'])
            print(
                f"üì¢ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {item['title']}\n"
                f"üîó –°—Å—ã–ª–∫–∞: {item['link']}\n"
                f"üìÖ –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {item['date']}\n"
                f"{'-' * 50}"
            )


if __name__ == "__main__":
    asyncio.run(main())