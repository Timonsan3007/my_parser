import aiohttp
import asyncio
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urljoin
from config import KEYWORDS, EXCLUDED_KEYWORDS


def is_valid_news(title):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å: —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞"""
    title_lower = title.lower()
    return (
            any(keyword.lower() in title_lower for keyword in KEYWORDS) and
            not any(excluded.lower() in title_lower for excluded in EXCLUDED_KEYWORDS)
    )


async def fetch_news_datetime(session, news_url):
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏"""
    try:
        async with session.get(news_url, ssl=False) as response:
            response.raise_for_status()
            soup = BeautifulSoup(await response.text(), "lxml")

            # –ò—â–µ–º –º–µ—Ç–∞-—Ç–µ–≥ —Å –≤—Ä–µ–º–µ–Ω–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            meta_time = soup.find("meta", {"property": "article:published_time"})
            if meta_time and meta_time.get("content"):
                return datetime.fromisoformat(meta_time["content"][:19])

            # –ï—Å–ª–∏ –º–µ—Ç–∞-—Ç–µ–≥ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏—â–µ–º —Ç–µ–≥ <time>
            date_tag = soup.find("time")
            if date_tag and date_tag.get("datetime"):
                return datetime.fromisoformat(date_tag["datetime"][:19])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")

    return None


async def fetch_v1_news():
    """–ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å–∞–π—Ç–∞ V1.ru –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏."""
    base_url = "https://v1.ru/"  # –°–∞–π—Ç V1.ru
    news_list = []
    yesterday = datetime.now().date() - timedelta(days=1)
    headers = {"User-Agent": "Mozilla/5.0"}
    timeout = aiohttp.ClientTimeout(total=60)

    async with aiohttp.ClientSession(headers=headers, timeout=timeout) as session:
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –≤—á–µ—Ä–∞—à–Ω–∏–π –¥–µ–Ω—å
            url = f"{base_url}?dateFrom={yesterday.strftime('%d.%m.%Y')}"
            async with session.get(url, ssl=False) as response:
                response.raise_for_status()
                soup = BeautifulSoup(await response.text(), "lxml")

                # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
                articles = soup.find_all("a", href=True)
                for article in articles:
                    title = article.text.strip()
                    link = urljoin(base_url, article["href"])

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å—Å—ã–ª–∫–∏
                    if not link.startswith(base_url):
                        continue

                    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–∏
                    news_date = await fetch_news_datetime(session, link)
                    if (
                            news_date and
                            news_date.date() >= yesterday and
                            is_valid_news(title)
                    ):
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        if not any(news['title'] == title for news in news_list):
                            news_list.append({
                                "title": title,
                                "link": link,
                                "date": news_date
                            })
        except aiohttp.ClientError as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ —Å–∞–π—Ç—É: {e}")

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–æ—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º)
    news_list.sort(key=lambda x: x["date"], reverse=True)

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É –¥–ª—è –≤—ã–≤–æ–¥–∞
    for news in news_list:
        news["date"] = news["date"].strftime("%d.%m.%y %H:%M")

    return news_list


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π."""
    v1_news = await fetch_v1_news()
    if not v1_news:
        print(f"‚ùóÔ∏è –î–ª—è —Å–∞–π—Ç–∞ https://v1.ru –Ω–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º!")
        print(f"{'-' * 50}")
        return

    seen_links = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    seen_titles = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    for news in v1_news:
        if news['link'] not in seen_links and news['title'] not in seen_titles:
            seen_links.add(news['link'])
            seen_titles.add(news['title'])
            print(
                f"üì¢ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {news['title']}\n"
                f"üîó –°—Å—ã–ª–∫–∞: {news['link']}\n"
                f"üìÖ –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {news['date']}\n"
                f"{'-' * 50}"
            )


if __name__ == "__main__":
    asyncio.run(main())